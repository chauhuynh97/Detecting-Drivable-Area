{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 720)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Sandesh\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def LoadSceneAndLabel(dirnameScene, dirnameLabel, numImages):\n",
    "    count = 0\n",
    "    sceneX = []\n",
    "    sceneY = []\n",
    "    for sceneName in os.listdir(dirnameScene):\n",
    "        scene = Image.open(os.path.join(dirnameScene, sceneName))\n",
    "        sceneLabel = Image.open(os.path.join(dirnameLabel, sceneName[:-4]+\"_drivable_color.png\"))\n",
    "        #scene.show()\n",
    "        #sceneLabel.show()\n",
    "        #print(scene.size)\n",
    "        #print(sceneLabel.size)\n",
    "        sceneX.append(scene)\n",
    "        sceneY.append(sceneLabel)\n",
    "        count += 1\n",
    "        if count >= numImages:\n",
    "          break\n",
    "    return sceneX, sceneY\n",
    "\n",
    "dataX, dataY = LoadSceneAndLabel(dirnameScene=\"bdd100k/images/100k/train\", dirnameLabel=\"bdd100k/drivable_maps/color_labels/train\", numImages=500)\n",
    "valX, valY = LoadSceneAndLabel(dirnameScene=\"bdd100k/images/100k/val\", dirnameLabel=\"bdd100k/drivable_maps/color_labels/val\", numImages=20)\n",
    "\n",
    "dataX[0].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 720)\n",
      "(1280, 720)\n",
      "(128, 72)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def compress(x, y, rate=0.5):\n",
    "    print((x[0].size))\n",
    "    xSize = math.floor(((x[0].size)[0])*rate)\n",
    "    ySize = math.floor(((y[0].size)[1])*rate)\n",
    "#     print(xSize)\n",
    "#     print(ySize)\n",
    "    sceneX = [img.resize([xSize, ySize]) for img in x]  \n",
    "    sceneY = [img.resize([xSize, ySize]) for img in y]  \n",
    "    #sceneX[0].show()\n",
    "    #sceneY[0].show()\n",
    "    return sceneX, sceneY\n",
    "\n",
    "# #print(SceneLabelPair[0][0].size)\n",
    "dataX, dataY = compress(dataX, dataY, rate=0.1)\n",
    "# print(SceneLabelPair[0][0].size)\n",
    "valX, valY = compress(valX, valY, rate=0.1)\n",
    "\n",
    "print(dataX[0].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maybe flatten y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#convert non zero label to be the same number so it's not different based on different lane categorization\n",
    "\n",
    "def convert_to_array(x, y):\n",
    "    sceneX = []\n",
    "    sceneY = []\n",
    "    count = 0\n",
    "    for img in x:\n",
    "        img = img.convert('LA')  # convert to grayscale\n",
    "        img = np.squeeze(np.array(img)[:,:,0])\n",
    "        #divide by 255.0\n",
    "        img = np.divide(img, 255.0)\n",
    "        sceneX.append(img)\n",
    "    for img in y:\n",
    "        img = img.convert('LA')  # convert to grayscale\n",
    "        if count == 0:\n",
    "            img.show()\n",
    "        count = 1\n",
    "        img = np.squeeze(np.array(img)[:,:,0])\n",
    "        img = np.where(img > 0, 1, 0)\n",
    "        sceneY.append(img)\n",
    "#     img = np.squeeze(np.array(img)[:, :, 0]) #squeeze to remove the unnecessary dimension after conversion to grayscale\n",
    "#     label_img = np.squeeze(np.array(label_img)[:, :, 0])\n",
    "    return np.array(sceneX), np.array(sceneY)\n",
    "\n",
    "dataX, dataY = convert_to_array(dataX, dataY)\n",
    "valX, valY = convert_to_array(valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500, 72, 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataX))\n",
    "print(len(dataY))\n",
    "\n",
    "\n",
    "dataX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split = 2/7.0\n",
    "print(split)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(dataX, dataY, test_size=split) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = trainX.shape[0]\n",
    "test_num = testX.shape[0]\n",
    "x_dim = trainX.shape[2]\n",
    "y_dim = trainX.shape[1]\n",
    "val_num = valX.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch CNN Model\n",
    "\n",
    "Module is the baseclass for all NN module:\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "### Conv2D -> \n",
    "Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "https://cs231n.github.io/convolutional-networks/\n",
    "out_channels = number of kernels (filters) used, so dim in the 3rd dim\n",
    "\n",
    "\n",
    "### BatchNorm2d ->\n",
    "BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "Batch normalization normalizes the activations of the network between layers in batches so that the batches have a mean of 0 and a variance of 1.\n",
    "param = num of channels\n",
    "\n",
    "### nn.Linear ->\n",
    "Linear(in_features, out_features, bias=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to PyTorch types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.94901961 0.95294118 0.95294118 ... 0.47843137 0.50588235 0.49019608]\n",
      "  [0.95294118 0.94901961 0.95294118 ... 0.47058824 0.45882353 0.49411765]\n",
      "  [0.91764706 0.96078431 0.94901961 ... 0.51764706 0.26666667 0.2745098 ]\n",
      "  ...\n",
      "  [0.1254902  0.10588235 0.08627451 ... 0.10588235 0.10196078 0.10196078]\n",
      "  [0.03137255 0.02745098 0.02745098 ... 0.23529412 0.22352941 0.22745098]\n",
      "  [0.0745098  0.08235294 0.09803922 ... 0.46666667 0.35686275 0.29411765]]]\n",
      "(357, 1, 72, 128)\n",
      "torch.Size([357, 9216])\n",
      "[[[0.42745098 0.42745098 0.2745098  ... 0.03137255 0.02745098 0.02745098]\n",
      "  [0.41960784 0.34901961 0.28235294 ... 0.03137255 0.02745098 0.02745098]\n",
      "  [0.40784314 0.38823529 0.18823529 ... 0.03529412 0.03137255 0.02745098]\n",
      "  ...\n",
      "  [0.10588235 0.10196078 0.09019608 ... 0.36862745 0.36470588 0.36078431]\n",
      "  [0.02352941 0.02352941 0.02352941 ... 0.35686275 0.35294118 0.35294118]\n",
      "  [0.02745098 0.02745098 0.02745098 ... 0.34901961 0.34901961 0.34509804]]]\n",
      "(20, 1, 72, 128)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# train\n",
    "\n",
    "trainX = trainX.reshape(train_num, 1, y_dim, x_dim).astype(float)\n",
    "print(trainX[0])\n",
    "print(trainX.shape)\n",
    "trainX  = torch.from_numpy(trainX).type(torch.FloatTensor)\n",
    "\n",
    "# trainY = trainY.reshape(train_num, 1, x_dim, y_dim).astype(float)\n",
    "# trainY  = torch.from_numpy(trainY)\n",
    "# print(trainY.shape)\n",
    "\n",
    "trainY = trainY.reshape(train_num, x_dim * y_dim).astype(float)\n",
    "#trainY  = torch.from_numpy(trainY).type(torch.LongTensor)\n",
    "trainY  = torch.from_numpy(trainY).type(torch.FloatTensor)\n",
    "print(trainY.shape)\n",
    "\n",
    "\n",
    "\n",
    "# val\n",
    "valX = valX.reshape(val_num, 1, y_dim, x_dim).astype(float)\n",
    "print(valX[0])\n",
    "print(valX.shape)\n",
    "valX  = torch.from_numpy(valX).type(torch.FloatTensor)\n",
    "\n",
    "# valY = valY.reshape(val_num, 1, x_dim, y_dim).astype(float)\n",
    "# valY  = torch.from_numpy(valY)\n",
    "\n",
    "\n",
    "valY = valY.reshape(val_num, x_dim * y_dim).astype(float)\n",
    "#valY  = torch.from_numpy(valY).type(torch.LongTensor)\n",
    "valY  = torch.from_numpy(valY).type(torch.FloatTensor)\n",
    "# //TODO Test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([357, 1, 72, 128]), torch.Size([357, 9216]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 72, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear, ReLU, MSELoss, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, Flatten\n",
    "\n",
    "# calculating the size of output from convolution layers\n",
    "dat = trainX[:1].type(torch.FloatTensor)\n",
    "print(dat.size())\n",
    "#x = dat.to(device=device, dtype=torch.float)\n",
    "conv = Conv2d(1, 2, kernel_size=3, stride=1, padding=1)\n",
    "out = conv(dat)\n",
    "mp = MaxPool2d(kernel_size=2, stride=2)\n",
    "out = mp(out)\n",
    "conv2 = Conv2d(2, 3, kernel_size=5, stride=1, padding=1)\n",
    "out = conv2(out)\n",
    "bN = BatchNorm2d(3)\n",
    "out = bN(out)\n",
    "mp2 = MaxPool2d(kernel_size=3, stride=2)\n",
    "out = mp2(out)\n",
    "out.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, Softmax, Sigmoid, ReLU, NLLLoss, MSELoss, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, Flatten\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__() #base class for all neural network modules\n",
    "        out_chan_1 = 2\n",
    "        out_chan_2 = 3\n",
    "        k_s_1 = 3\n",
    "        k_s_2 = 5\n",
    "        k_s_1_mp = 2 #mp = max pool\n",
    "        k_s_2_mp = 2\n",
    "        \n",
    "        num_neurons_1 = 10\n",
    "        \n",
    "        output_size = 9216 #depending on num of pixels\n",
    "        \n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, out_chan_1, kernel_size=k_s_1, stride=1, padding=1), \n",
    "            BatchNorm2d(out_chan_1), #just specify the num of output channels\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=k_s_1_mp, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(out_chan_1, out_chan_2, kernel_size=k_s_2, stride=1, padding=1),\n",
    "            BatchNorm2d(out_chan_2),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=k_s_2_mp, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(out_chan_2 * 31 * 17, num_neurons_1),\n",
    "            Softmax(dim=1),\n",
    "            #ReLU(inplace=True),\n",
    "            Linear(num_neurons_1,output_size),\n",
    "            Sigmoid() # map from 0 to 1\n",
    "            #Softmax(dim=1)\n",
    "            #Sign()\n",
    "            #ReLU(inplace=True)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.cnn_layers(x)\n",
    "        print(x.size())\n",
    "        #x=x.long()\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        #x = x.float()\n",
    "        return x\n",
    "    \n",
    "\n",
    "# defining the model\n",
    "model = CNN()\n",
    "# defining the optimizer\n",
    "optimizer =  torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.5, weight_decay=0.0005)\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# defining the loss function\n",
    "criterion = MSELoss()\n",
    "#criterion = CrossEntropyLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "# print(model)\n",
    "\n",
    "# trainX = trainX[:2].cuda()\n",
    "\n",
    "# output = model(Variable(trainX))\n",
    "# output\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch, train_x, train_y, val_x, val_y):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # getting the training set\n",
    "    x_train, y_train = Variable(train_x), Variable(train_y)\n",
    "    # getting the validation set\n",
    "    x_val, y_val = Variable(val_x), Variable(val_y)\n",
    "    # converting the data into GPU format\n",
    "    if torch.cuda.is_available():\n",
    "        x_train = x_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        x_val = x_val.cuda()\n",
    "        y_val = y_val.cuda()\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    print(output_train.shape)\n",
    "    print(y_train.shape)\n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    loss_val = criterion(output_val, y_val)\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  1 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  3 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  5 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  7 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  9 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  11 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  13 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  15 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  17 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  19 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  21 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  23 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  25 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  27 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  29 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  31 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  33 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  35 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  37 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  39 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  41 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  43 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  45 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  47 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n",
      "Epoch :  49 \t loss : tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "torch.Size([357, 3, 17, 31])\n",
      "torch.Size([20, 3, 17, 31])\n",
      "torch.Size([357, 9216])\n",
      "torch.Size([357, 9216])\n"
     ]
    }
   ],
   "source": [
    "# defining the number of epochs\n",
    "n_epochs = 50\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "# training the model\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch, trainX, trainY, valX, valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt \n",
    "# plt.plot(train_losses, label='Training loss')\n",
    "# plt.plot(val_losses, label='Validation loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#discussion with Chau\n",
    "\n",
    "should label data be 0 and 1 per pixel rather than whatever it currently is?\n",
    "should we normalize all the images to be closer to 1 rather than such large numbers?\n",
    "should just discussion on loss function -> i think mseloss would be best for pixel by pixel comparison?\n",
    "should final output be filtered using something other than nerual network?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 17, 31])\n",
      "(9216,)\n",
      "[[0.51428044 0.43378332 0.57502514 ... 0.4416776  0.4225259  0.49711508]]\n",
      "[[  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.   0.   0. ... 255. 255. 255.]\n",
      " [  0.   0.   0. ... 255. 255. 255.]\n",
      " [  0.   0.   0. ... 255. 255. 255.]]\n",
      "[[255   0 255 ... 255   0 255]\n",
      " [  0   0   0 ... 255   0 255]\n",
      " [  0 255   0 ... 255   0 255]\n",
      " ...\n",
      " [255 255   0 ... 255 255   0]\n",
      " [255   0 255 ... 255   0   0]\n",
      " [255 255 255 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "testSingle = valX[0:1]\n",
    "#testSingle = testSingle.cuda()\n",
    "y_pred = model(testSingle)\n",
    "\n",
    "\n",
    "out = y_pred.cpu().detach().numpy()\n",
    "\n",
    "print(valY[0].numpy().shape)\n",
    "print(out)\n",
    "\n",
    "final = np.where(out > np.mean(out), 1, 0)\n",
    "\n",
    "final = np.reshape(final, (72, 128))*255\n",
    "\n",
    "actual = ((np.reshape(valY[0].numpy(), (72, 128)))*255)\n",
    "\n",
    "print(actual)\n",
    "\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd08a64160>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(actual, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd08a28e80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(final, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
